# A comparison of LSTM and BERT for small corpus, Ezen-Can, 2020

## [Paper](https://arxiv.org/abs/2009.05451), Tags: \#nlp, \#architectures

Given a small dataset, can we use a large pre-trained model like BERT and get better results than simple models? We use a small dataset and compare the performance of a simple biLSTM with a pre-trained BERT model on text classification using chatbot data and trying to predict its intent. Our results show that biLSTM achieves significantly higher results than a BERT model.

We use a small dataset of 24k utterances and build several LSTM models from scratch. The simplest LSTM architecture we tried works best for this dataset. 

We conclude that the performance of a model is dependent on the task and the data.
