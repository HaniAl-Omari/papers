# Beyond accuracy: behavioral testing of NLP models with CheckList, Ribeiro et al., 2020

## [Paper](https://arxiv.org/abs/2005.04118), [Code](https://github.com/marcotcr/checklist), Tags: \#nlp

CheckList is a task-agnostic methodology for testing NLP models. It includes a matrix of general lingistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for 3 tasks, identifying critical failures in commercial and state-of-the-art models.

Using the classic train-validation-test paradigm, test data usually has the same biases as the training data. Accuracy on benchmarks is not sufficient for evaluating NLP models.

Users "checklist" a model by filling out cells in a matrix, each cell potentially containing multiple tests. Rows are capabilities and columns are test types. CheckList treats the model as a black box, allowing the comparison of different models trained on different data.

We suggest that users consider at least the following capabilities. See examples on table 1 in paper.

* Vocabulary + POS
* Taxonomy
* NER
* Fairness
* Temporal
* Negation
* Coreference
* Semantic role labeling
* Logic

We suggest users evaluate each capability with three different test types:

* Minimum functionality test (MFT)
    - it's a collection of simple examples and labels to check a behavior within a capability. Like unit testing in software development
* Invariance (INV)
    - Apply label-preserving perturbations to inputs and expect the model prediction to remain the same
* Directional expectation tests (DIR)
    - similar to previous one but the label is expected to change

We provide templates to scape up creation from scratch and make perturbations easier to craft.

> I {negation} {positive verb} the {thing}

We apply CheckList to various commercial applications, and there are various failures on many task-relevant linguistic capabilities.
