# Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data, Bender et al., 2020

## [Paper](https://www.aclweb.org/anthology/2020.acl-main.463/), Tags: \#nlp, \#linguistics

We argue that a system exposed only to form in its training cannot in principle learn meaning. It lacks the ability to connect its utterances to the world. This paper is a call for precise language use when talking about the success of current models and for humility in dealing with natural language.

> We define language model as any system trained only on the task of string prediction, operating on characters, words, sentences, and whether sequential or not. We take meaning as the relation between a linguistic form and communicative intent.

## Large LMs: hype and analysis

Some researchers have found that instead from understanding language, large LMs falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original.

Large LMs can learn aspects of linguistic formal structure (agreement, dependency structure) and their apparent ability to 'reason' is sometimes a mirage built on leveraging artifacts in the training data (form, not meaning).

## What is meaning?

Form: any observable realization of language; marks on a page, pixels or bytes in a digital representation of text, etc. Meaning: relation between the form and something external to language.

Meaning is obtained by joining the natural language expression (e) and the communicative intent (i). Understanding is the process of retrieving i given e.

The speaker has a certain communicative intent i, and chooses an expression e with a meaning s which is fit to express i in the current communicative situation. Upon hearing e, the listener then reconstructs s and uses their own knowledge of the communicative situation and their hypotheses about the speakerâ€™s state of mind and intention in an attempt to deduce i.

If a model is purely trained on form, there is no sufficient signal to learn the relation between that form and the non-linguistic intent of human language users.

For humans or machines to learn language, they must solve the *symbol grounding problem*: it is not possible for a non-speaker of Chinese to learn the meanings of Chinese words from Chinese dictionary definitions alone.

See paper section 4 for the octopus test. The octopus only fooled A because A is such an active listener. Because agents who produce English sentences usually have communicative intents, A assumes that O does too, and thus A builds the conventional meaning ENglish associates with the octopus' utterances. She uses that conventional meaning with her other guesses about B's state of mind and goals to attribute communicative intent. The octopus' utterances do not make sense, but A can make sense of them.

## Human language acquisition

One common reason for believing LMs *might* be learning meaning is the claim that human children can acquire language just by listening to it. This is not supported by scholarly work on language acquisition: human language learning is not only grounded in the physical world around us, but also in interaction with other people in that world. **Human children do not learn meaning from form alone and we should not expect machines to do so either.**

One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. (2020).

More about top-down, bottom-up theory building in NLP on section 8 in paper.
