# Interpretable Machine Learning - A Brief History, State-of-the-Art and Challenges, Molnar et al., 2020

## [Paper](https://arxiv.org/abs/2010.09337), Tags: \#machine-learning

Interpretability is a deciding factor when a ML model is used in a product, a decision process, or in research. IML methods can be used to discover knowledge, debug or justify the model and its predictions, and to control and improve the model.

We distinguish IML method by whether they analyze model components, model sensitivity or surrogate models.

### Analyzing components of interpretable models

Component analysis is always model-specific, because it is tied to the structure of the model. Inherently interpretable models are models with learned structures and learned parameters which can be assigned a certain interpretation. Linear regression models, decision trees and decision rules are considered to be interpretable.

If the scenario is more high dimensional, these deep morels are not interpretable anymore.

### Analyzing components of more complex models

The abstract features learned by a deep CNN can be visualized by finding or generating images that activate a feature map of the CNN.

### Explaining individual predictions

These IML methods often treat ML models as a closed system that receives feature values as an input and produces a prediction as output. There can be local and global explanations.

Local methods explain individual predictions of ML models, and some of them rely on model-specific knowledge to analyze how changes in the input features change the output. Saliency maps are an interpretation method used for CNNs which make use of the network gradients to explain individual classifications.

### Explaining global model behavior

Global model-agnostic explanation methods are used to explain the expected model behavior, how the model behaves on average for a given dataset.

### Surrogate models

They are interpretable models designed to 'copy' the behavior of the ML model. It treats the ML model as a black-box and only requires the input and output data of the ML model to train a surrogate ML model.

## Challenges of IML

### Statistical uncertainty and inference

Many IML methods do not quantify the uncertainty of the explanations they give. The model itself and also its explanations are computed from data and hence are subject to uncertainty.

### Causal interpretation

Ideally, a model should reflect the true causal structure of its underlying phenomena, to enable causal interpretations. But most statistical learning procedures reflect mere correlation structures between features and analyze the surface of the data generation process instead of its true inherent structure.

### Feature dependence

See paper section 5.3.

### Definition of interpretability

It is not so clear what the definition of interpretability is, there is no ground truth explanation and there is no straightforward way to quantify how interpretable a model is or how correct an explanation is.

The two main ways of evaluating interpretability are objective evaluations, which are mathematically quantifiable metrics, and human-centered evaluations, which invove studies with either domain experts or lay persons.
