# Generalization through Memorization: Nearest Neighbor Language Models, Khandelwal et al., 2019

## [Paper](https://arxiv.org/abs/1911.00172), Tags: \#nlp, \#architecture

We introduce kNN-LMs, which extend a pre-trained neural language model by linearly interpolating it with a k-nearest neighbor model. The nearest neighbors are computed according to the distance in the pre-trained LM embedding space.

Our kNN-LM achieves a new stateof-the-art perplexity of 15.79 â€“ a 2.9 point improvement with no additional training. Our approach can efficiently scale up to larger training sets and allows for effective domain adaptation by simply varying the nearest neighbors datastore, without further training. The model is particularly helpful in finding rare patterns such as factual konwledge.

Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.

This approach allows rare patterns to be memorized explicitly, rather than implicitly in model parameters.

The datastore contains an entry for each target in the training set, which for LMs can be up to billions of examples. To search over this large database, we use FAISS, an open source library for fast NN retrieval in high dimensional spaces. FAISS speeds up search by clustering the keys and looking up neighbors based on the cluster centroids, while reducing memory usage by storing compressed versions of the vector.

As a model we use decoder-only Transformers for language modeling. LMs are trained to minimize the negative log likelihood and evaluated on perplexity.

For the kNN part, we perform a single forward pass over the training set with the trained model, in order to save the keys and values.

## Analysis

Retrieving neighbors from the training data can significantly improve language modeling performance --> can retrieving nearest neighbors from ata be a substitute for training on it? The model trained on 3B tokens dramatically outperforms the model trained on 100M tokens, improving perplexity from 19.59 to 15.17. But adding NN retrieval over those 3B tokens to the model trained on 100M tokens improves perplexity from 19.59 to 13.73: **retrieving NNs from the corpus outperforms training on it**. This result suggests that rather than training language models on ever larger datasets, we can use smaller datasets to learn representations and augment them with kNN-LM over a large corpus.
