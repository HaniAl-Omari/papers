# Unsupervised neural machine translation, Artetxe et al., 2017

## [Paper](https://arxiv.org/abs/1710.11041), [Repo](https://github.com/artetxem/undreamt), Tags: \#nlp, \#machine-translation

We propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. It builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation.

For each sentence in L1, the system is trained alternating two steps:

* denoising: optimizes the probability of encoding a noised version of the sentence with the shared encoder and reconstructs it with the L1 decoder
    - we incorporate noise in the input language and alter the word order randomly. We discourage the system to rely too much on the word order of the input sequence, so we account for the actual word order divergences across languages.
* on-the-fly backtranslation: translates the sentence in inference mode, encoding it with the shared encoder and decoding it with the L2 decoder
    - given an input sentence in L1, we use the system in inference mode with greedy decoding to translate it to the other language. We obtain a pseudo-parallel sentence pair, and we train the system to predict the original sentence from this synthetic translation. We take advantage of the dual structure of the proposed architecture to backtranslate each mini-batch on-the-fly using the same model. As training progresses and the model improves, it will produce better synthetic sentence pairs through backtranslation, which will serve to further improve the model in the following iterations.

We use pre-trained cross-lingual embeddings in the encoder that are kept fixed during training. The encoder is given language independent word-level representations, and it only needs to learn how to compose them to build representations of larger phrases.

In each iteration, we do this

* one mini-batch of denoising for L1
* same for L2
* one mini-batch of on-the-fly backtranslation from L1 to L2
* same from L2 to L1

We use BPE in preprocessing, which has good results with ENG-DEU, worse in ENG-FRA. A subword unit sometimes gets prefixed to a properly translated word, yielding the translation like *SevAgency* (split as S- ev- Agency). BPE is also of little help when translating infrequent named entities. Our system translates Tymoshenk as Ebferchenko (split as Eb- fer- chenko). These relations are much more challenging in our unsuperivsed learning procedure.

We also show that the proposed model can greatly benefit from a small parallel corpus. Our system goes beyond a literal word-by-word translation, and handles structural differences between languages well.

But our system quality often lags behind that of a standard supervised NMT system. Our model has difficulties preserving some concrete details from source sentences. We believe that incorporating character level information might help mitigate some of these issues. There are also some cases where there are both fluency and adequacy problems that severely hinder understanding the original message from the proposed translation.
